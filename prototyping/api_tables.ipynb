{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from time import sleep\n",
    "\n",
    "urls = {\n",
    "    'api' : 'https://api.mtga.untapped.gg/api/v1/',\n",
    "    'json' : 'https://mtgajson.untapped.gg/v1/latest/',\n",
    "}\n",
    "\n",
    "endpoints = {\n",
    "    'active': ('api', 'meta-periods/active'),\n",
    "    'analytics': ('api', 'analytics/query/card_stats_by_archetype_event_and_scope_free/ALL?MetaPeriodId='),\n",
    "    'cards': ('json', 'cards.json'),\n",
    "    'text': ('json', 'loc_en.json'),\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'authority': 'api.mtga.untapped.gg',\n",
    "    'accept': '*/*',\n",
    "    'accept-language': 'en-US,en;q=0.9,pt;q=0.8',\n",
    "    'if-none-match': '\"047066ff947f01e9e609ca4cf0d6c0a6\"',\n",
    "    'origin': 'https://mtga.untapped.gg',\n",
    "    'referer': 'https://mtga.untapped.gg/',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"111\", \"Not(A:Brand\";v=\"8\", \"Chromium\";v=\"111\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'cors',\n",
    "    'sec-fetch-site': 'same-site',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache\n",
    "def request(keyword, send_headers=True, format=''):\n",
    "    'Returns JSON from corresponding keyword'\n",
    "    url_kw, endpoint = endpoints[keyword]\n",
    "    url = urls[url_kw]\n",
    "    \n",
    "    sleep(1) # :)\n",
    "    if send_headers:\n",
    "        return requests.get(url+endpoint+format, headers).json()\n",
    "    else:\n",
    "        return requests.get(url+endpoint+format).json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355 ['MID', 'VOW', 'NEO', 'SNC', 'DMU', 'BRO', 'ONE', 'MOM']\n"
     ]
    }
   ],
   "source": [
    "def request_active():\n",
    "    'Returns format ID and lists of standard legal sets'\n",
    "    latest = dict()\n",
    "\n",
    "    # Extracting information from the lastest standard BO1 format\n",
    "    for format in request('active'):\n",
    "        if format['event_name'] == 'Ladder':\n",
    "            latest = format\n",
    "\n",
    "    return str(latest['id']), latest['legal_sets']\n",
    "\n",
    "format_id, legal_sets = request_active()\n",
    "print(format_id, legal_sets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_cards(sets):\n",
    "    'Returns transformed cards dataframe'\n",
    "    df = pd.DataFrame(request('cards'))\n",
    "\n",
    "    # Ony bother with standard legal cards\n",
    "    df = df[df.set.isin(sets)]\n",
    "\n",
    "    # Remove duplicates by considering only the latest reprint\n",
    "    gb = df.groupby('titleId').agg({'grpid':'max'})\n",
    "    df = df[df.grpid.isin(gb.grpid)]\n",
    "\n",
    "    return df.set_index('grpid')\n",
    "\n",
    "raw_card = request_cards(legal_sets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_text():\n",
    "    'Returns card text dataframe'\n",
    "    df = pd.DataFrame(request('text')).set_index('id')\n",
    "\n",
    "    # Collapse columns raw and text, prioritizing raw\n",
    "    df.loc[~df.raw.isna(), 'text'] = df.raw\n",
    "    df.drop('raw', axis='columns', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "raw_text = request_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Card information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_raw_card(raw_card):\n",
    "    'Remove undesirable columns, change naming conventions and change index to titleId'\n",
    "    \n",
    "    # Columns to ignore and corresponding reasoning\n",
    "    ignore = [\n",
    "        'artistCredit',                  # Deemed not useful for analysis     \n",
    "        'collectorNumber',               # Deemed not useful for analysis\n",
    "        'collectorMax',                  # Deemed not useful for analysis\n",
    "        'types',                         # Aggregation from other columns\n",
    "        'subtypes',                      # Aggregation from other columns\n",
    "        'colors',                        # Aggregation from other columns\n",
    "        'colorIdentity',                 # Aggregation from other columns\n",
    "        'frameColors',                   # Deemed not useful for analysis\n",
    "        'supertypes',                    # Aggregation from other columns\n",
    "        'rawFrameDetails',               # Deemed not useful for analysis\n",
    "        'altDeckLimit',                  # Not standard relevant\n",
    "        'DigitalReleaseSet',             # Not standard relevant\n",
    "        'frameDetails',                  # Deemed not useful for analysis\n",
    "        'abilityIdToLinkedTokenGrpId',   # Not standard relevant\n",
    "        'linkedFaceType',                # Deemed not useful for analysis\n",
    "        'linkedFaces',                   # Deemed not useful for analysis\n",
    "        'usesSideboard',                 # Not standard relevant\n",
    "        'IsDigitalOnly',                 # Not standard relevant\n",
    "        'watermark',                     # Deemed not useful for analysis\n",
    "        'RebalancedCardLink',            # Deemed not useful for analysis\n",
    "        'altTitleId',                    # Deemed not useful for analysis\n",
    "        'DefunctRebalancedCardLink',     # Deemed not useful for analysis\n",
    "        'abilityIdToLinkedConjurations', # Not standard relevant\n",
    "        'grpid',                         # Obsolete, as titleId is now unique\n",
    "        'hiddenAbilities',               # Already contained in 'abilities'\n",
    "    ]\n",
    "\n",
    "    # Naming conventions\n",
    "    # raw_card.index.rename('card_id', inplace=True)\n",
    "    raw_card.rename({\n",
    "        'set':'set_id',\n",
    "        'isSecondaryCard': 'is_secondary_card',\n",
    "        'isToken': 'is_token',\n",
    "        'IsRebalanced': 'is_rebalanced',\n",
    "        'artId': 'art_id'},\n",
    "        axis = 'columns',\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    # Set titleId to index and drop undesirable columns\n",
    "    return raw_card.reset_index().drop(ignore, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_dataframe(df):\n",
    "    \"Returns card dataframe\"   \n",
    "\n",
    "    # Card columns mappable to text\n",
    "    id_to_text = ['titleId', 'flavorId', 'cardTypeTextId', 'subtypeTextId']\n",
    "\n",
    "    # Additional text_columns columns\n",
    "    df = df.join(\n",
    "        df[id_to_text]\n",
    "        .applymap(lambda x: raw_text.loc[x].values[0], na_action='ignore')\n",
    "        .rename(columns={column: column[:-2] for column in id_to_text}) # Remove Id\n",
    "    )\n",
    "\n",
    "    # Only tracked supertype will be 'legendary'\n",
    "    df['is_legendary'] = df.cardTypeText.str.contains('Legendary')\n",
    "\n",
    "    # Replace empty string flavor with NaN\n",
    "    df['flavor'] = df['flavor'].replace({'': np.nan})\n",
    "\n",
    "    # Define the columns and order for card dataframe\n",
    "    order = [\n",
    "        'titleId',\n",
    "        'art_id',\n",
    "        'set_id',\n",
    "        'title',\n",
    "        'rarity',\n",
    "        'power',\n",
    "        'toughness',\n",
    "        'flavor',\n",
    "        'is_legendary',\n",
    "        'is_token',\n",
    "        'is_secondary_card',\n",
    "        'is_rebalanced',\n",
    "    ]\n",
    "\n",
    "    return df[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_type_dataframe(df):\n",
    "    'Returns card type dataframe'\n",
    "    # Convert 'cardTypeTextId' to text\n",
    "    df = df.join(\n",
    "        df.subtypeTextId\n",
    "        .map(lambda x: raw_text.loc[x].values[0], na_action='ignore')\n",
    "        .rename('type')\n",
    "    )\n",
    "    \n",
    "    # Split the text and transform into a list of rows, deleting the ones without information\n",
    "    df = df.type.str.split().explode().dropna()\n",
    "\n",
    "    # There are special cases of cards not having types such as cards 'Day' and 'Night'\n",
    "    df = df[~df.isin(['NONE', 'Legendary', 'Basic', 'Token'])]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_subtype_dataframe(df):\n",
    "    \"Returns card subtype dataframe\"\n",
    "    # Convert 'cardTypeTextId' to text\n",
    "    df = df.join(\n",
    "        df.subtypeTextId\n",
    "        .map(lambda x: raw_text.loc[x].values[0], na_action='ignore')\n",
    "        .rename('subtype') \n",
    "    )\n",
    "\n",
    "    # Split the text and transform into a list of rows, deleting the ones without information\n",
    "    df = df.subtype.str.split().explode().dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_cost(df):\n",
    "    \"Returns card cost dataframe\"\n",
    "\n",
    "    # Casting color codes in 'castingcost' column\n",
    "    casting_colors = {\n",
    "        'Black': r'oB',\n",
    "        'Blue': r'oU',\n",
    "        'Green': r'oG',\n",
    "        'Red': r'oR',\n",
    "        'White': r'oW',\n",
    "        'Multicolor': r'\\(',\n",
    "        'X': r'oX'\n",
    "    }\n",
    "\n",
    "    # Create a column for each variety\n",
    "    for color, code in casting_colors.items():\n",
    "        df[color] = df.castingcost.str.count(code)\n",
    "\n",
    "    # Special case is colorless that can be any number\n",
    "    df['Colorless'] = df.castingcost.str.extract('(\\d+)')\n",
    "\n",
    "    # Stack columns into rows and set index to card_id only\n",
    "    df = (df[['Colorless'] + list(casting_colors.keys())].stack()\n",
    "                                                         .reset_index()\n",
    "                                                         .set_index('card_id')\n",
    "                                                         .rename(columns={'level_1': 'color', 0: 'cost'}))\n",
    "\n",
    "    df.cost = pd.to_numeric(df.cost) # Convert from string to number\n",
    "\n",
    "    # Only record costs above zero\n",
    "    return df[df.cost > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_ability(df):\n",
    "    df = (df.abilities\n",
    "            .dropna()\n",
    "            .explode()\n",
    "            .apply(lambda x: x.get('TextId'))\n",
    "            .map(lambda x: raw_text.loc[x].values[0]))      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_information():\n",
    "    'Returns multiple dataframes containing card information after normalization'\n",
    "    filtered = filter_raw_card(raw_card)\n",
    "    card = get_card_dataframe(filtered)\n",
    "\n",
    "    # Rename 'titleId' to 'card_id' and promote it to index\n",
    "    for df in [filtered, card]:\n",
    "        df.set_index('titleId', inplace=True)\n",
    "        df.index.name = 'card_id'\n",
    "\n",
    "    card_type = get_card_type_dataframe(filtered)\n",
    "    card_subtype = get_card_subtype_dataframe(filtered)\n",
    "    card_cost = get_card_cost(filtered)\n",
    "    card_ability = get_card_ability(filtered)\n",
    "\n",
    "    return filtered, card, card_type, card_subtype, card_cost, card_ability\n",
    "\n",
    "filtered, card, card_type, card_subtype, card_cost, card_ability = get_card_information()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 11576, 'g': 40253, 'p': 299962, 's': 27606}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def request_analytics(format):\n",
    "    json = request('analytics', False, format)\n",
    "\n",
    "    # Separate into data and medatada\n",
    "    metadata = json['metadata']\n",
    "    df = pd.json_normalize(json['data']).T\n",
    "\n",
    "    # Number of games per tier\n",
    "    games = {tier[0]: metadata['games'][tier]['ALL'] \n",
    "             for tier in metadata['games']}\n",
    "\n",
    "    # Nested list with two levels\n",
    "    level_1 = ['games', 'wins', 'check', 'copies']\n",
    "    level_2 = ['copies_1', 'copies_2', 'copies_3', 'copies_4']\n",
    "\n",
    "    # Unnest the list in stages\n",
    "    # When we have a new set, sometimes Untapped chooses to mix BO1 and BO3 statistics\n",
    "    # Thus we make sure to only get BO1 statistics for consistency.\n",
    "    df[level_1] = pd.DataFrame(df.explode(0)[0].to_list(), index=df.index).iloc[:, :4]\n",
    "    df[level_2] = pd.DataFrame(df.copies.to_list(), index=df.index)\n",
    "\n",
    "    # Remove redundant columns and fill NaN with zeros \n",
    "    df.drop([0, 'check', 'copies'], axis = 'columns', inplace = True)\n",
    "    df.fillna(0, inplace = True)\n",
    "\n",
    "    # Transform dtypes to int, reset index and rename tiers\n",
    "    df = (df.astype('int64')\n",
    "            .reset_index()\n",
    "            .rename(columns={'index':'raw'}))\n",
    "    \n",
    "    # Split raw column into multiple\n",
    "    df[['titleId', 'archetypeId', 'tier']] = df.raw.str.split('.', expand=True)\n",
    "\n",
    "    # Record only consolidated data by archetype and change index to titleId\n",
    "    df = (df[df.archetypeId == 'ALL']\n",
    "            .drop(['raw', 'archetypeId'], axis=1)\n",
    "            .set_index('titleId'))\n",
    "    df.index = df.index.astype('int64')\n",
    "    \n",
    "    # Time stamp and format\n",
    "    df.insert(0, 'dt_analytics', pd.Timestamp.today().strftime('%Y-%m-%d'))\n",
    "#     df.insert(1, 'format', format)\n",
    "    \n",
    "    return df, games\n",
    "\n",
    "analytics, no_games = request_analytics(format_id)\n",
    "no_games"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'titleId'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 46\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m         \u001b[39mreturn\u001b[39;00m merged[[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     38\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mset\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     39\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mrarity\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mincluded\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     44\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mquantity\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m---> 46\u001b[0m df_abt \u001b[39m=\u001b[39m abt(format_id, legal_sets)\n",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m, in \u001b[0;36mabt\u001b[1;34m(format, sets, tier, full_report)\u001b[0m\n\u001b[0;32m     18\u001b[0m     analytics[\u001b[39m'\u001b[39m\u001b[39mincluded\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m analytics\u001b[39m.\u001b[39mgames\u001b[39m/\u001b[39mgames[tier]\n\u001b[0;32m     20\u001b[0m \u001b[39m# Merge analytics and card information\u001b[39;00m\n\u001b[0;32m     21\u001b[0m merged \u001b[39m=\u001b[39m (analytics\u001b[39m.\u001b[39;49mreset_index()\n\u001b[1;32m---> 22\u001b[0m                    \u001b[39m.\u001b[39;49mmerge(card_information\u001b[39m.\u001b[39;49mreset_index(),\n\u001b[0;32m     23\u001b[0m                           how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m     24\u001b[0m                           on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtitleId\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     25\u001b[0m                    \u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mtitleId\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     27\u001b[0m \u001b[39m# Winrate\u001b[39;00m\n\u001b[0;32m     28\u001b[0m merged[\u001b[39m'\u001b[39m\u001b[39mwinrate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m merged\u001b[39m.\u001b[39mwins\u001b[39m/\u001b[39mmerged\u001b[39m.\u001b[39mgames\n",
      "File \u001b[1;32mc:\\Bruno\\Projects\\MTGA-s-standard-meta-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:9843\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   9824\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   9825\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m   9826\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9839\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   9840\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m   9841\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge\u001b[39;00m \u001b[39mimport\u001b[39;00m merge\n\u001b[1;32m-> 9843\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[0;32m   9844\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   9845\u001b[0m         right,\n\u001b[0;32m   9846\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m   9847\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m   9848\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m   9849\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m   9850\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m   9851\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m   9852\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   9853\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m   9854\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   9855\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m   9856\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m   9857\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Bruno\\Projects\\MTGA-s-standard-meta-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:142\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    141\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m--> 142\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    143\u001b[0m         left,\n\u001b[0;32m    144\u001b[0m         right,\n\u001b[0;32m    145\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m    146\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m    147\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m    148\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m    149\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m    150\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m    151\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    152\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m    153\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m    154\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    155\u001b[0m     )\n\u001b[0;32m    156\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Bruno\\Projects\\MTGA-s-standard-meta-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:731\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cross \u001b[39m=\u001b[39m cross_col\n\u001b[0;32m    726\u001b[0m \u001b[39m# note this function has side effects\u001b[39;00m\n\u001b[0;32m    727\u001b[0m (\n\u001b[0;32m    728\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[0;32m    729\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[0;32m    730\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[1;32m--> 731\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_merge_keys()\n\u001b[0;32m    733\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m    735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[1;32mc:\\Bruno\\Projects\\MTGA-s-standard-meta-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1189\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m rk \u001b[39m=\u001b[39m cast(Hashable, rk)\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m rk \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1189\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39;49m_get_label_or_level_values(rk))\n\u001b[0;32m   1190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1191\u001b[0m     \u001b[39m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Bruno\\Projects\\MTGA-s-standard-meta-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:1778\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1776\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mget_level_values(key)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1777\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1780\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'titleId'"
     ]
    }
   ],
   "source": [
    "def abt(format, sets, tier='ALL', full_report=False):\n",
    "    \"Full dataframe for a given query\"\n",
    "    card_information, *_= get_card_information()\n",
    "    analytics, games = request_analytics(format)\n",
    "\n",
    "    # Choose either aggregate statistics per tier or single one\n",
    "    # Use if clause to calculate 'included' statistic\n",
    "    if tier == 'ALL': # Aggregate games statistics\n",
    "        agg_dict = {column : 'sum' \n",
    "                    if column not in ['dt_analytics', 'format', 'tier']\n",
    "                    else 'max'\n",
    "                    for column in analytics.columns}\n",
    "        analytics = analytics.groupby('titleId').agg(agg_dict)\n",
    "        analytics.tier = 'ALL'\n",
    "        analytics['included'] = analytics.games/sum(games.values())\n",
    "    else:\n",
    "        analytics = analytics[analytics.tier == tier]\n",
    "        analytics['included'] = analytics.games/games[tier]\n",
    "\n",
    "    # Merge analytics and card information\n",
    "    merged = (analytics.reset_index()\n",
    "                       .merge(card_information.reset_index(),\n",
    "                              how='left', \n",
    "                              on='titleId')\n",
    "                       .set_index('titleId'))\n",
    "    \n",
    "    # Winrate\n",
    "    merged['winrate'] = merged.wins/merged.games\n",
    "\n",
    "    # Quantity\n",
    "    copies = ['copies_1', 'copies_2', 'copies_3', 'copies_4']\n",
    "    merged['quantity'] = merged[copies].idxmax(axis=1)\n",
    "    \n",
    "    if full_report:\n",
    "        return merged\n",
    "    else:\n",
    "        return merged[['title',\n",
    "                       'set',\n",
    "                       'rarity', \n",
    "                       'castingcost', \n",
    "                       'winrate', \n",
    "                       'games', \n",
    "                       'included', \n",
    "                       'quantity']]\n",
    "    \n",
    "df_abt = abt(format_id, legal_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
